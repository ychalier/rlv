{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Overview\n",
    "\n",
    "In this notebook, we test out several new features using an [ExtraTrees classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html) from the scikit-learn library. In particular, we will do the following:\n",
    "\n",
    "* Define a function for cross-validation and generating test predictions\n",
    "* Get a baseline using the original data with no feature engineering\n",
    "* Consolidate prior feature engineering from other sources\n",
    "* Create new features from the soil type columns\n",
    "* Create a submission combining all new features\n",
    "\n",
    "The main contribution of this notebook is in the testing of features based on the soil type. Using the [information given](https://archive.ics.uci.edu/ml/machine-learning-databases/covtype/covtype.info) with the original dataset we can extract features from the soil type which improve the cross-validation accuracy. You can find all of my work using this dataset on [github](https://github.com/rsizem2/forest-cover-type-prediction)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables for testing changes to this notebook quickly\n",
    "RANDOM_SEED = 0\n",
    "NUM_FOLDS = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "## 1. Imports\n",
    "\n",
    "Import all relevant libraries, models, evaluation metrics, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import scipy\n",
    "import time\n",
    "import pyarrow\n",
    "import gc\n",
    "\n",
    "# Model & Evaluation\n",
    "from functools import partial\n",
    "from sklearn.base import clone\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data\n",
    "\n",
    "Load training/test data and encode the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load original data\n",
    "train = pd.read_csv('input/train.csv')\n",
    "test = pd.read_csv('input/test.csv')\n",
    "submission = pd.read_csv('input/sampleSubmission.csv')\n",
    "\n",
    "# Label Encode\n",
    "encoder = LabelEncoder()\n",
    "train[\"Cover_Type\"] = encoder.fit_transform(train[\"Cover_Type\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Scoring Function\n",
    "\n",
    "Function for performing k-fold cross-validation and averaging the test predictions across each fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    \n",
    "    print(f'{NUM_FOLDS}-fold Cross Validation\\n')\n",
    "    \n",
    "    # Train/Test split\n",
    "    X_temp = train[features]\n",
    "    X_test = test[features]\n",
    "    y_temp = train['Cover_Type']\n",
    "    \n",
    "    # Store the out-of-fold predictions\n",
    "    test_preds = np.zeros((X_test.shape[0],7))\n",
    "    oof_preds = np.zeros((X_temp.shape[0],))\n",
    "    fi_scores = np.zeros((X_temp.shape[1],))\n",
    "    scores, times = np.zeros(NUM_FOLDS), np.zeros(NUM_FOLDS)\n",
    "    \n",
    "    # Stratified k-fold cross-validation\n",
    "    skf = StratifiedKFold(n_splits = NUM_FOLDS, shuffle = True, random_state = RANDOM_SEED)\n",
    "    for fold, (train_idx, valid_idx) in enumerate(skf.split(X_temp,y_temp)):\n",
    "       \n",
    "        # Training and Validation Sets\n",
    "        X_train, X_valid = X_temp.iloc[train_idx], X_temp.iloc[valid_idx]\n",
    "        y_train, y_valid = y_temp.iloc[train_idx], y_temp.iloc[valid_idx]\n",
    "        \n",
    "        # Create model\n",
    "        start = time.time()\n",
    "        model = ExtraTreesClassifier(\n",
    "            n_estimators = 118,\n",
    "            min_samples_split = 2,\n",
    "            max_features = 14,\n",
    "            random_state = RANDOM_SEED,\n",
    "            n_jobs = -1,\n",
    "        )\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # validation/holdout predictions\n",
    "        valid_preds = np.ravel(model.predict(X_valid))\n",
    "        oof_preds[valid_idx] = valid_preds\n",
    "        test_preds += model.predict_proba(X_test)\n",
    "\n",
    "        # Save scores and times\n",
    "        scores[fold] = accuracy_score(y_valid, valid_preds)\n",
    "        end = time.time()\n",
    "        times[fold] = end-start\n",
    "        print(f'Fold {fold}: {round(scores[fold], 5)} in {round(times[fold], 2)}s')\n",
    "        time.sleep(0.5)\n",
    "    \n",
    "    test_preds = np.argmax(test_preds, axis = 1)\n",
    "    print('\\nModel: '+model.__class__.__name__)\n",
    "    print(\"Train Accuracy:\", round(scores.mean(), 5))\n",
    "    print(f'Training Time: {round(times.sum(), 2)}s')\n",
    "    \n",
    "    return test_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Submission\n",
    "\n",
    "We start by training a model on the original data without any feature engineering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12-fold Cross Validation\n",
      "\n",
      "Fold 0: 0.88571 in 2.43s\n",
      "Fold 1: 0.8881 in 2.54s\n",
      "Fold 2: 0.87937 in 2.52s\n",
      "Fold 3: 0.88095 in 2.54s\n",
      "Fold 4: 0.8754 in 2.43s\n",
      "Fold 5: 0.87222 in 2.47s\n",
      "Fold 6: 0.87778 in 2.43s\n",
      "Fold 7: 0.8746 in 2.4s\n",
      "Fold 8: 0.85794 in 2.43s\n",
      "Fold 9: 0.88651 in 2.51s\n",
      "Fold 10: 0.88413 in 2.49s\n",
      "Fold 11: 0.89048 in 2.5s\n",
      "\n",
      "Model: ExtraTreesClassifier\n",
      "Train Accuracy: 0.87943\n",
      "Training Time: 29.7s\n"
     ]
    }
   ],
   "source": [
    "# Baseline submission\n",
    "features = [x for x in train.columns if x not in ['Id', 'Cover_Type']] \n",
    "submission['Cover_Type'] = encoder.inverse_transform(\n",
    "    train_model()\n",
    ")\n",
    "submission.to_csv('baseline_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Feature Engineering\n",
    "\n",
    "In this section we consider several new features generated from the original data. Check out the following notebooks/discussions for more information and original sources for some of these features:\n",
    "\n",
    "* Notebook: [Two models Random Forests](https://www.kaggle.com/shouldnotbehere/two-models-random-forests)\n",
    "* Notebook: [my_first_submission](https://www.kaggle.com/jianyu/my-first-submission)\n",
    "* Discussion: [TPS12 feature engineering](https://www.kaggle.com/c/tabular-playground-series-dec-2021/discussion/293612)\n",
    "\n",
    "The features included below are those from the previous sources which resulted in improved CV accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def misc_features(data):\n",
    "    df = data.copy()\n",
    "    \n",
    "    # Use float64 for calculations\n",
    "    for col, dtype in df.dtypes.iteritems():\n",
    "        if dtype.name.startswith('float'):\n",
    "            df[col] = df[col].astype('float64')\n",
    "            \n",
    "    # Interaction Terms\n",
    "    df['Horizontal_Distance_To_Roadways_Log'] = [math.log(v+1) for v in df['Horizontal_Distance_To_Roadways']]\n",
    "    df['Water Elevation'] = df['Elevation'] - df['Vertical_Distance_To_Hydrology']\n",
    "    df['Hydro_Fire_1'] = df['Horizontal_Distance_To_Hydrology'] + df['Horizontal_Distance_To_Fire_Points']\n",
    "    df['Hydro_Fire_2'] = abs(df['Horizontal_Distance_To_Hydrology'] - df['Horizontal_Distance_To_Fire_Points'])\n",
    "    df['Hydro_Road_1'] = abs(df['Horizontal_Distance_To_Hydrology'] + df['Horizontal_Distance_To_Roadways'])\n",
    "    df['Hydro_Road_2'] = abs(df['Horizontal_Distance_To_Hydrology'] - df['Horizontal_Distance_To_Roadways'])\n",
    "    df['Fire_Road_1'] = abs(df['Horizontal_Distance_To_Fire_Points'] + df['Horizontal_Distance_To_Roadways'])\n",
    "    df['Fire_Road_2'] = abs(df['Horizontal_Distance_To_Fire_Points'] - df['Horizontal_Distance_To_Roadways'])\n",
    "    df['EHiElv'] = df['Horizontal_Distance_To_Roadways'] * df['Elevation']\n",
    "    df['EVDtH'] = df.Elevation - df.Vertical_Distance_To_Hydrology\n",
    "    df['EHDtH'] = df.Elevation - df.Horizontal_Distance_To_Hydrology * 0.2\n",
    "    df['Elev_3Horiz'] = df['Elevation'] + df['Horizontal_Distance_To_Roadways']  + df['Horizontal_Distance_To_Fire_Points'] + df['Horizontal_Distance_To_Hydrology']\n",
    "    df['Elev_Road_1'] = df['Elevation'] + df['Horizontal_Distance_To_Roadways']\n",
    "    df['Elev_Road_2'] = df['Elevation'] - df['Horizontal_Distance_To_Roadways']\n",
    "    df['Elev_Fire_1'] = df['Elevation'] + df['Horizontal_Distance_To_Fire_Points']\n",
    "    df['Elev_Fire_2'] = df['Elevation'] - df['Horizontal_Distance_To_Fire_Points']\n",
    "\n",
    "    # Fill NA\n",
    "    df.fillna(0, inplace = True)\n",
    "    \n",
    "    # Downcast variables\n",
    "    for col, dtype in df.dtypes.iteritems():\n",
    "        if dtype.name.startswith('int'):\n",
    "            df[col] = pd.to_numeric(df[col], downcast ='integer')\n",
    "        elif dtype.name.startswith('float'):\n",
    "            df[col] = pd.to_numeric(df[col], downcast ='float')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# misc feature engineering\n",
    "train = misc_features(train)\n",
    "test = misc_features(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Soil Type Feature Engineering\n",
    "\n",
    "In this section we consider several new features based on the soil-type variables:\n",
    "\n",
    "0. Categorical Encoding (40 columns -> 1 column)\n",
    "1. Climatic Zone\n",
    "2. Geologic Zone\n",
    "3. Surface Cover\n",
    "4. Rock Size\n",
    "5. Interaction Terms\n",
    "6. Drop Original (dimension reduction)\n",
    "\n",
    "## ELU Codes\n",
    "\n",
    "From the original data description, the soil type number is based on the USFS Ecological Landtype Units (ELUs). The ELU code contains further information about the soils including the climatic zone and geologic zone. Furthermore, each ELU code comes with a brief description from which we can extract further information about the surface cover (by rocks/boulders) and rock size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping soil type to ELU code\n",
    "ELU_CODE = {\n",
    "    1:2702,2:2703,3:2704,4:2705,5:2706,6:2717,7:3501,8:3502,9:4201,\n",
    "    10:4703,11:4704,12:4744,13:4758,14:5101,15:5151,16:6101,17:6102,\n",
    "    18:6731,19:7101,20:7102,21:7103,22:7201,23:7202,24:7700,25:7701,\n",
    "    26:7702,27:7709,28:7710,29:7745,30:7746,31:7755,32:7756,33:7757,\n",
    "    34:7790,35:8703,36:8707,37:8708,38:8771,39:8772,40:8776\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Categorical Encoding\n",
    "\n",
    "**Note:** Soil type is a single variable which has been one-hot encoded presumably to behave nicely with a neural network. Generally, tree-based models work better without explicit one-hot encoding, so we will reverse engineer the soil type. We will eventually drop the original soil type columns which has the added effect of significantly reducing the total number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode soil type ordinally\n",
    "def categorical_encoding(input_df):\n",
    "    data = input_df.copy()\n",
    "    data['Soil_Type'] = 0\n",
    "    for i in range(1,41):\n",
    "        data['Soil_Type'] += i*data[f'Soil_Type{i}']\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode soil type\n",
    "train = categorical_encoding(train)\n",
    "test = categorical_encoding(test)\n",
    "\n",
    "# Original soil features\n",
    "soil_features = [f'Soil_Type{i}' for i in range(1,41)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Climatic Zone (Ordinal Variable)\n",
    "\n",
    "We create a feature based on the climatic zone of the soil. This can be determined by the first digit of the ELU code. Note that the climatic zone has a natural ordering:\n",
    "\n",
    "1. lower montane dry\n",
    "2. lower montane\n",
    "3. montane dry\n",
    "4. montane\n",
    "5. montane dry and montane\n",
    "6. montane and subalpine\n",
    "7. subalpine\n",
    "8. alpine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def climatic_zone(input_df):\n",
    "    df = input_df.copy()\n",
    "    df['Climatic_Zone'] = input_df['Soil_Type'].apply(\n",
    "        lambda x: int(str(ELU_CODE[x])[0])\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Climatic Zone\n",
    "train = climatic_zone(train)\n",
    "test = climatic_zone(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Geologic Zone (Nominal Variable)\n",
    "\n",
    "This is another feature that comes directly from the ELU code. The geologic zone is determined by the second digit in the ELU code. Unlike the climatic zone, the geologic zone has no natural ordering:\n",
    "\n",
    "1. alluvium\n",
    "2. glacial\n",
    "3. shale\n",
    "4. sandstone\n",
    "5. mixed sedimentary\n",
    "6. unspecified in the USFS ELU Survey\n",
    "7. igneous and metamorphic\n",
    "8. volcanic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geologic_zone(input_df):\n",
    "    df = input_df.copy()\n",
    "    df['Geologic_Zone'] = input_df['Soil_Type'].apply(\n",
    "        lambda x: int(str(ELU_CODE[x])[1])\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Geologic Zone\n",
    "train = geologic_zone(train)\n",
    "test = geologic_zone(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Surface Cover (Ordinal Variable)\n",
    "\n",
    "This feature is also based on the ELU code and the original data description for each soil type. Note that not all of the soil types have a description of their surface cover. According to the [USDA reference](https://www.nrcs.usda.gov/wps/portal/nrcs/detail/soils/ref/?cid=nrcs142p2_054253#surface_fragments) on soil profiling:\n",
    "\n",
    "1. (Stony/Bouldery) — Stones or boulders cover 0.01 to less than 0.1 percent of the surface. The smallest stones are at least 8 meters apart; the smallest boulders are at least 20 meters apart.\n",
    "\n",
    "2. (Very Stony/Very Bouldery) — Stones or boulders cover 0.1 to less than 3 percent of the surface. The smallest stones are not less than 1 meter apart; the smallest boulders are not less than 3 meters apart.\n",
    "\n",
    "3. (Extremely Stony/Extremely Bouldery) — Stones or boulders cover 3 to less than 15 percent of the surface. The smallest stones are as little as 0.5 meter apart; the smallest boulders are as little as 1 meter apart.\n",
    "\n",
    "4. (Rubbly) — Stones or boulders cover 15 to less than 50 percent of the surface. The smallest stones are as little as 0.3 meter apart; the smallest boulders are as little as 0.5 meter apart. In most places it is possible to step from stone to stone or jump from boulder to boulder without touching the soil.\n",
    "\n",
    "5. (Very Rubbly) — Stones or boulders appear to be nearly continuous and cover 50 percent or more of the surface. The smallest stones are less than 0.03 meter apart; the smallest boulders are less than 0.05 meter apart. Classifiable soil is among the rock fragments, and plant growth is possible.\n",
    "\n",
    "If no description of the surface cover is given, we give it a value of 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def surface_cover(input_df):\n",
    "    # Group IDs\n",
    "    no_desc = [7,8,14,15,16,17,19,20,21,23,35]\n",
    "    stony = [6,12]\n",
    "    very_stony = [2,9,18,26]\n",
    "    extremely_stony = [1,22,24,25,27,28,29,30,31,32,33,34,36,37,38,39,40]\n",
    "    rubbly = [3,4,5,10,11,13]\n",
    "\n",
    "    # Create dictionary\n",
    "    surface_cover = {i:0 for i in no_desc}\n",
    "    surface_cover.update({i:1 for i in stony})\n",
    "    surface_cover.update({i:2 for i in very_stony})\n",
    "    surface_cover.update({i:3 for i in extremely_stony})\n",
    "    surface_cover.update({i:4 for i in rubbly})\n",
    "    \n",
    "    # Create Feature\n",
    "    df = input_df.copy()\n",
    "    df['Surface_Cover'] = input_df['Soil_Type'].apply(\n",
    "        lambda x: surface_cover[x]\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Surface Cover\n",
    "train = surface_cover(train)\n",
    "test = surface_cover(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Rock Size (Nominal)\n",
    "\n",
    "The final soil type feature we consider is rock size. This is also determined from the ELU code, the original data description and the [USFS soil profiling reference](https://www.nrcs.usda.gov/wps/portal/nrcs/detail/soils/ref/?cid=nrcs142p2_054253#fragments):\n",
    "\n",
    "1. Stones\n",
    "2. Boulders\n",
    "3. Rubble\n",
    "\n",
    "If the soil type description has no mention of rock size, we give it a default value of 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rock_size(input_df):\n",
    "    \n",
    "    # Group IDs\n",
    "    no_desc = [7,8,14,15,16,17,19,20,21,23,35]\n",
    "    stones = [1,2,6,9,12,18,24,25,26,27,28,29,30,31,32,33,34,36,37,38,39,40]\n",
    "    boulders = [22]\n",
    "    rubble = [3,4,5,10,11,13]\n",
    "\n",
    "    # Create dictionary\n",
    "    rock_size = {i:0 for i in no_desc}\n",
    "    rock_size.update({i:1 for i in stones})\n",
    "    rock_size.update({i:2 for i in boulders})\n",
    "    rock_size.update({i:3 for i in rubble})\n",
    "    \n",
    "    df = input_df.copy()\n",
    "    df['Rock_Size'] = input_df['Soil_Type'].apply(\n",
    "        lambda x: rock_size[x]\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Surface Cover\n",
    "train = rock_size(train)\n",
    "test = rock_size(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Soil Type Interactions\n",
    "\n",
    "In this section we form some interaction features using soil type and these new soil-type derived features. We include only those features which resulted in improved CV accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soiltype_interactions(data):\n",
    "    df = data.copy()\n",
    "            \n",
    "    # Important Soil Types\n",
    "    df['Soil_12_32'] = df['Soil_Type32'] + df['Soil_Type12']\n",
    "    df['Soil_Type23_22_32_33'] = df['Soil_Type23'] + df['Soil_Type22'] + df['Soil_Type32'] + df['Soil_Type33']\n",
    "    \n",
    "    # Soil Type Interactions\n",
    "    df['Soil29_Area1'] = df['Soil_Type29'] + df['Wilderness_Area1']\n",
    "    df['Soil3_Area4'] = df['Wilderness_Area4'] + df['Soil_Type3']\n",
    "    \n",
    "    #  New Feature Interactions\n",
    "    df['Climate_Area2'] = df['Wilderness_Area2']*df['Climatic_Zone'] \n",
    "    df['Climate_Area4'] = df['Wilderness_Area4']*df['Climatic_Zone'] \n",
    "    df['Rock_Area1'] = df['Wilderness_Area1']*df['Rock_Size']    \n",
    "    df['Rock_Area3'] = df['Wilderness_Area3']*df['Rock_Size']  \n",
    "    df['Surface_Area1'] = df['Wilderness_Area1']*df['Surface_Cover'] \n",
    "    df['Surface_Area2'] = df['Wilderness_Area2']*df['Surface_Cover']   \n",
    "    df['Surface_Area4'] = df['Wilderness_Area4']*df['Surface_Cover'] \n",
    "    \n",
    "    # Fill NA\n",
    "    df.fillna(0, inplace = True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Surface Cover\n",
    "train = soiltype_interactions(train)\n",
    "test = soiltype_interactions(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Drop Columns (Dimension Reduction)\n",
    "\n",
    "Finally, we drop the original soil type columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop original soil features\n",
    "train.drop(columns = soil_features, inplace = True)\n",
    "test.drop(columns = soil_features, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12-fold Cross Validation\n",
      "\n",
      "Fold 0: 0.90952 in 2.4s\n",
      "Fold 1: 0.91746 in 2.44s\n",
      "Fold 2: 0.90476 in 2.41s\n",
      "Fold 3: 0.90476 in 2.47s\n",
      "Fold 4: 0.90873 in 2.46s\n",
      "Fold 5: 0.91111 in 2.42s\n",
      "Fold 6: 0.91349 in 2.39s\n",
      "Fold 7: 0.91508 in 2.41s\n",
      "Fold 8: 0.90476 in 2.39s\n",
      "Fold 9: 0.91508 in 2.52s\n",
      "Fold 10: 0.91032 in 2.64s\n",
      "Fold 11: 0.90952 in 2.6s\n",
      "\n",
      "Model: ExtraTreesClassifier\n",
      "Train Accuracy: 0.91038\n",
      "Training Time: 29.55s\n"
     ]
    }
   ],
   "source": [
    "# Submission with feature engineering\n",
    "features = [x for x in train.columns if x not in ['Id', 'Cover_Type']] \n",
    "submission['Cover_Type'] = encoder.inverse_transform(\n",
    "    train_model()\n",
    ")\n",
    "submission.to_csv('features_submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit (system)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5de252128c3e90b5ab77a2537560095409d5071d8d2efe6bc04d622ca931c6ef"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
